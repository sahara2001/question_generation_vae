{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0718 20:11:54.748471 139764970538752 deprecation_wrapper.py:119] From /home1/shifangping/miniconda3/envs/vae/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0718 20:11:54.767318 139764970538752 deprecation_wrapper.py:119] From /home1/shifangping/miniconda3/envs/vae/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0718 20:11:54.770374 139764970538752 deprecation_wrapper.py:119] From /home1/shifangping/miniconda3/envs/vae/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0718 20:11:54.895626 139764970538752 deprecation_wrapper.py:119] From /home1/shifangping/miniconda3/envs/vae/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n",
      "109\n",
      "length 188\n",
      "<class 'numpy.int64'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0718 20:11:55.062997 139764970538752 deprecation_wrapper.py:119] From /home1/shifangping/miniconda3/envs/vae/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0718 20:11:55.074424 139764970538752 deprecation_wrapper.py:119] From /home1/shifangping/miniconda3/envs/vae/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0718 20:11:55.163876 139764970538752 deprecation.py:323] From /home1/shifangping/miniconda3/envs/vae/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 10, 64)       12032       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gcnn_1 (GCNN)                   (None, 10, 64)       24576       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gcnn_2 (GCNN)                   (None, 10, 64)       24576       gcnn_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gcnn_3 (GCNN)                   (None, 10, 64)       24576       gcnn_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gcnn_4 (GCNN)                   (None, 10, 64)       24576       gcnn_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gcnn_5 (GCNN)                   (None, 10, 64)       24576       gcnn_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gcnn_6 (GCNN)                   (None, 10, 64)       24576       gcnn_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gcnn_7 (GCNN)                   (None, 10, 64)       24576       gcnn_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gcnn_8 (GCNN)                   (None, 10, 64)       24576       gcnn_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gcnn_9 (GCNN)                   (None, 10, 64)       24576       gcnn_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gcnn_10 (GCNN)                  (None, 10, 64)       24576       gcnn_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           gcnn_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           4160        global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           4160        global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 64)           0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 640)          41600       lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 10, 64)       0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gcnn_11 (GCNN)                  (None, 10, 64)       24576       reshape_1[0][0]                  \n",
      "                                                                 gcnn_11[0][0]                    \n",
      "                                                                 gcnn_11[1][0]                    \n",
      "                                                                 gcnn_11[2][0]                    \n",
      "                                                                 gcnn_11[3][0]                    \n",
      "                                                                 gcnn_11[4][0]                    \n",
      "                                                                 gcnn_11[5][0]                    \n",
      "                                                                 gcnn_11[6][0]                    \n",
      "                                                                 gcnn_11[7][0]                    \n",
      "                                                                 gcnn_11[8][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 10, 188)      12220       gcnn_11[9][0]                    \n",
      "==================================================================================================\n",
      "Total params: 344,508\n",
      "Trainable params: 344,508\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "109/109 [==============================] - 2s 20ms/step - loss: 113.6261\n",
      "b'          \\xe5\\xb3\\xb0\\xe4\\xb9\\x83\\xe6\\x8e\\xa5\\xe6\\x8e\\xa5\\xe5\\xbe\\x85\\xef\\xbc\\x8c\\xe7\\xbb\\xae\\xe5\\x96\\x84\\xe5\\xbe\\x85\\xe5\\xb8\\xad\\xe7\\x94\\xb5'\n",
      "Epoch 2/100\n",
      "109/109 [==============================] - 0s 263us/step - loss: 96.4633\n",
      "b'          \\xe8\\x8d\\xb7\\xe5\\xb3\\xb0\\xe7\\xb4\\xa0\\xe6\\x85\\x8e\\xe9\\x9f\\xb3\\xef\\xbc\\x8c\\xe6\\xb1\\xbe\\xe6\\x98\\x8e\\xe5\\xbe\\x80\\xe6\\x82\\xac\\xe9\\xa6\\x80'\n",
      "Epoch 3/100\n",
      "109/109 [==============================] - 0s 278us/step - loss: 87.1423\n",
      "b'          \\xe9\\xa6\\x80\\xe8\\x8d\\xb7\\xe6\\xa1\\xa5\\xe8\\x82\\x86\\xe6\\xb1\\x89\\xef\\xbc\\x8c\\xe7\\xbe\\x8e\\xe6\\xad\\xa4\\xe5\\x88\\x9d\\xe6\\xad\\x8c\\xe8\\xbf\\xb9'\n",
      "Epoch 4/100\n",
      "109/109 [==============================] - 0s 294us/step - loss: 77.9103\n",
      "b'          \\xe6\\xb7\\xb9\\xe8\\xa7\\x82\\xe9\\x87\\x8c\\xe5\\x87\\xba\\xe6\\xb3\\x9b\\xef\\xbc\\x8c\\xe5\\xb3\\xb0\\xe8\\xbe\\x99\\xe6\\xb1\\xbe\\xe5\\xa6\\x82\\xe5\\xae\\xab'\n",
      "Epoch 5/100\n",
      "109/109 [==============================] - 0s 269us/step - loss: 72.4446\n",
      "b'          \\xe6\\xa1\\xa5\\xe9\\xa6\\x99\\xe9\\x83\\x91\\xe4\\xb9\\x83\\xe8\\x8a\\xb3\\xef\\xbc\\x8c\\xe4\\xb8\\xba\\xe5\\xaf\\xb8\\xe8\\x99\\x9a\\xe7\\x8e\\xb3\\xe7\\x8e\\x89'\n",
      "Epoch 6/100\n",
      "109/109 [==============================] - 0s 310us/step - loss: 65.1631\n",
      "b'          \\xe5\\x90\\xaf\\xe5\\xaf\\x9f\\xe6\\x96\\x9c\\xe5\\x8a\\xb3\\xe9\\x87\\x91\\xef\\xbc\\x8c\\xe9\\x83\\x91\\xe5\\x87\\x9d\\xe5\\xa4\\x9a\\xe8\\x8a\\xb3\\xe6\\xb3\\x9b'\n",
      "Epoch 7/100\n",
      "109/109 [==============================] - 0s 301us/step - loss: 63.3137\n",
      "b'          \\xe4\\xba\\xba\\xe7\\xad\\xb5\\xe5\\xaf\\xb8\\xe6\\x91\\x87\\xe8\\xaf\\x9a\\xef\\xbc\\x8c\\xe9\\x9f\\xb3\\xe8\\x99\\x9a\\xe6\\x82\\xac\\xe5\\x9c\\x83\\xe9\\x9b\\x95'\n",
      "Epoch 8/100\n",
      "109/109 [==============================] - 0s 290us/step - loss: 60.2404\n",
      "b'          \\xe6\\x95\\xb7\\xe5\\x8d\\xb1\\xe6\\xac\\xa2\\xe6\\x97\\xa2\\xe6\\x91\\x87\\xef\\xbc\\x8c\\xe6\\x8e\\xa5\\xe5\\x85\\xb9\\xe5\\xaf\\x9f\\xe7\\xbb\\xb3\\xe5\\xb9\\xbf'\n",
      "Epoch 9/100\n",
      "109/109 [==============================] - 0s 307us/step - loss: 58.6942\n",
      "b'          \\xe6\\x96\\x9c\\xe6\\xad\\x8c\\xe8\\x90\\x8d\\xe7\\xaf\\x86\\xe7\\x96\\x91\\xef\\xbc\\x8c\\xe5\\x85\\xb8\\xe4\\xb8\\xbe\\xe7\\x96\\x91\\xe8\\xb5\\x8f\\xe9\\x9f\\xb3'\n",
      "Epoch 10/100\n",
      "109/109 [==============================] - 0s 293us/step - loss: 57.4256\n",
      "b'          \\xe7\\x83\\x9f\\xe4\\xbd\\x95\\xe9\\xaa\\x8f\\xe5\\xa4\\x84\\xe5\\x96\\x84\\xef\\xbc\\x8c\\xe5\\x87\\x9d\\xe6\\xae\\xbd\\xe6\\xb7\\xb9\\xe8\\x8d\\xb7\\xe9\\xa6\\x99'\n",
      "Epoch 11/100\n",
      "109/109 [==============================] - 0s 305us/step - loss: 56.5077\n",
      "b'          \\xe7\\xbb\\xb3\\xe7\\x83\\x9f\\xe8\\xba\\xac\\xe9\\xa6\\x99\\xe4\\xb8\\xba\\xef\\xbc\\x8c\\xe6\\xb8\\x85\\xe4\\xbb\\x99\\xe8\\x8a\\xac\\xe5\\x8f\\xaf\\xe6\\x98\\xad'\n",
      "Epoch 12/100\n",
      "109/109 [==============================] - 0s 261us/step - loss: 55.5329\n",
      "b'          \\xe8\\xb5\\x8f\\xe9\\x99\\x88\\xe9\\x9f\\xb3\\xe6\\x97\\xa0\\xe7\\xaf\\x86\\xef\\xbc\\x8c\\xe5\\x96\\x84\\xe5\\x96\\x84\\xe6\\x97\\xa5\\xe6\\x97\\xa2\\xe8\\xb5\\x8f'\n",
      "Epoch 13/100\n",
      "109/109 [==============================] - 0s 219us/step - loss: 54.6715\n",
      "b'          \\xe4\\xbd\\x95\\xe9\\x87\\x91\\xe5\\xaf\\x9f\\xe6\\x8e\\xa5\\xe5\\x8c\\x96\\xef\\xbc\\x8c\\xe7\\x83\\x9f\\xe9\\x98\\x85\\xe6\\x81\\xb6\\xe5\\xbf\\x98\\xe9\\xa6\\x80'\n",
      "Epoch 14/100\n",
      "109/109 [==============================] - 0s 298us/step - loss: 53.9049\n",
      "b'          \\xe5\\xb3\\xb0\\xe7\\x83\\x9f\\xe7\\xa6\\xbb\\xe5\\x9d\\x9f\\xe5\\xbf\\x85\\xef\\xbc\\x8c\\xe6\\xae\\xbf\\xe8\\xa7\\x82\\xe5\\x88\\x9d\\xe9\\xa3\\x8e\\xe6\\xb8\\x85'\n",
      "Epoch 15/100\n",
      "109/109 [==============================] - 0s 319us/step - loss: 53.0326\n",
      "b'          \\xe9\\x80\\x9a\\xe8\\xaf\\x9a\\xe6\\xa1\\xa5\\xe5\\x8d\\xab\\xe5\\x88\\x9d\\xef\\xbc\\x8c\\xe7\\x96\\x8f\\xe6\\xb1\\xbe\\xe6\\xb1\\x89\\xe5\\xa6\\x82\\xe8\\x8a\\xb3'\n",
      "Epoch 16/100\n",
      "109/109 [==============================] - 0s 308us/step - loss: 52.3996\n",
      "b'          \\xe5\\x85\\xb9\\xe9\\x9b\\xaa\\xe7\\xbb\\xae\\xe5\\x87\\xba\\xe9\\x9b\\x89\\xef\\xbc\\x8c\\xe7\\x8e\\xb3\\xe4\\xb8\\xba\\xe5\\x90\\xaf\\xe5\\xbd\\xa9\\xe5\\xb2\\x82'\n",
      "Epoch 17/100\n",
      "109/109 [==============================] - 0s 280us/step - loss: 51.9690\n",
      "b'          \\xe7\\xaf\\x86\\xe7\\x96\\x91\\xe5\\x85\\xb9\\xe5\\x97\\xa3\\xe5\\xb0\\xba\\xef\\xbc\\x8c\\xe9\\xa3\\x8e\\xe8\\xb0\\x8f\\xe5\\xae\\xb4\\xe6\\xb1\\xbe\\xe4\\xb8\\xbe'\n",
      "Epoch 18/100\n",
      "109/109 [==============================] - 0s 309us/step - loss: 51.2975\n",
      "b'          \\xe6\\xb1\\xbe\\xe6\\x9b\\xb2\\xe5\\x8d\\xab\\xe6\\xb2\\xb3\\xe5\\x8a\\xb3\\xef\\xbc\\x8c\\xe8\\xb0\\x8f\\xe5\\xaf\\xbb\\xe7\\x9b\\x88\\xe8\\x8d\\xb7\\xe4\\xb8\\xba'\n",
      "Epoch 19/100\n",
      "109/109 [==============================] - 0s 299us/step - loss: 49.9425\n",
      "b'          \\xe8\\x8d\\xa1\\xe9\\x95\\xbf\\xe4\\xbd\\x95\\xe7\\xad\\xb5\\xe9\\x9b\\xaa\\xef\\xbc\\x8c\\xe5\\xbf\\x97\\xe4\\xb8\\xbe\\xe8\\xb5\\x8f\\xe6\\xad\\xa4\\xe9\\x87\\x91'\n",
      "Epoch 20/100\n",
      "109/109 [==============================] - 0s 312us/step - loss: 48.6113\n",
      "b'          \\xe8\\xa7\\x88\\xe9\\xa6\\x99\\xe5\\x87\\xba\\xe5\\x8d\\xb1\\xe9\\x80\\x9a\\xef\\xbc\\x8c\\xe5\\x85\\xb8\\xe6\\x82\\xa6\\xe5\\xaf\\xbb\\xe6\\x82\\xac\\xe5\\xb9\\xbf'\n",
      "Epoch 21/100\n",
      "109/109 [==============================] - 0s 269us/step - loss: 48.0962\n",
      "b'          \\xe5\\xa4\\x9a\\xe7\\x99\\xbe\\xe6\\xb2\\xb3\\xe9\\x87\\x8c\\xe5\\xaf\\xb9\\xef\\xbc\\x8c\\xe6\\x91\\x87\\xe6\\xae\\xbd\\xe7\\xbd\\x97\\xe6\\xb1\\x89\\xe9\\xa9\\xac'\n",
      "Epoch 22/100\n",
      "109/109 [==============================] - 0s 315us/step - loss: 45.4511\n",
      "b'          \\xe9\\x87\\x8c\\xe6\\xb1\\xbe\\xe7\\xaf\\x86\\xe9\\x87\\x8d\\xe8\\xbe\\x99\\xef\\xbc\\x8c\\xe6\\xb2\\xb3\\xe7\\x83\\x9f\\xe7\\xa2\\xa7\\xe5\\xb1\\x82\\xe6\\x97\\xa0'\n",
      "Epoch 23/100\n",
      "109/109 [==============================] - 0s 309us/step - loss: 44.2470\n",
      "b'          \\xe6\\x98\\x8e\\xe5\\xbe\\x85\\xe4\\xb8\\x8a\\xe6\\xae\\xbf\\xe6\\xac\\xb9\\xef\\xbc\\x8c\\xe9\\xa3\\x8e\\xe8\\xbd\\xbb\\xe9\\x87\\x8c\\xe7\\xaf\\x86\\xe4\\xb9\\xb1'\n",
      "Epoch 24/100\n",
      "109/109 [==============================] - 0s 314us/step - loss: 42.8958\n",
      "b'          \\xe6\\xae\\xbf\\xe6\\x97\\xa5\\xe9\\x9f\\xb5\\xe5\\x85\\xb9\\xe7\\xa2\\xa7\\xef\\xbc\\x8c\\xe9\\x9b\\x89\\xe6\\x98\\x8e\\xe8\\x99\\x9a\\xe7\\x83\\x9f\\xe7\\x8e\\x89'\n",
      "Epoch 25/100\n",
      "109/109 [==============================] - 0s 285us/step - loss: 41.0592\n",
      "b'          \\xe4\\xb8\\x87\\xe6\\x97\\xa2\\xe5\\x87\\x9d\\xe8\\xb5\\x8f\\xe9\\x97\\xb4\\xef\\xbc\\x8c\\xe6\\x8e\\xa5\\xe7\\x91\\x81\\xe6\\xae\\xbd\\xe7\\xaf\\x86\\xe7\\xbd\\x97'\n",
      "Epoch 26/100\n",
      "109/109 [==============================] - 0s 230us/step - loss: 38.2875\n",
      "b'          \\xe5\\x89\\x8d\\xe7\\xbe\\x8e\\xe4\\xb9\\x83\\xe6\\xb1\\x89\\xe8\\xb5\\x8f\\xef\\xbc\\x8c\\xe5\\x85\\xb9\\xe5\\xaf\\xb9\\xe6\\x98\\xad\\xe7\\x92\\xa7\\xe5\\xb9\\xbf'\n",
      "Epoch 27/100\n",
      "109/109 [==============================] - 0s 243us/step - loss: 37.1582\n",
      "b'          \\xe5\\xbe\\x80\\xe5\\x9d\\x9f\\xe6\\xb7\\xb9\\xe7\\xbd\\x97\\xe9\\x80\\x9a\\xef\\xbc\\x8c\\xe5\\x9d\\x9f\\xe5\\x85\\xb9\\xe5\\xa4\\x9a\\xe5\\xbe\\x85\\xe5\\xaf\\x9f'\n",
      "Epoch 28/100\n",
      "109/109 [==============================] - 0s 298us/step - loss: 33.6894\n",
      "b'          \\xe5\\xb3\\xb0\\xe5\\x85\\xb9\\xe6\\xb7\\xb3\\xe6\\xb1\\x89\\xe5\\x87\\x9d\\xef\\xbc\\x8c\\xe4\\xb8\\x8a\\xe4\\xbb\\x99\\xe5\\x85\\xb0\\xe9\\x80\\x9a\\xe6\\x95\\xa3'\n",
      "Epoch 29/100\n",
      "109/109 [==============================] - 0s 308us/step - loss: 32.4840\n",
      "b'          \\xe4\\xb8\\xba\\xe6\\x96\\xb9\\xe5\\xbf\\x85\\xe9\\x98\\xb3\\xe9\\x80\\x9a\\xef\\xbc\\x8c\\xe8\\xb5\\xb7\\xe6\\x80\\x80\\xe9\\x83\\x91\\xe5\\x9c\\x83\\xe8\\xba\\xac'\n",
      "Epoch 30/100\n",
      "109/109 [==============================] - 0s 314us/step - loss: 30.0181\n",
      "b'          \\xe9\\x9b\\x89\\xe6\\xb1\\xbe\\xe5\\x8d\\xb1\\xe8\\xb5\\x8f\\xe5\\xbe\\x97\\xef\\xbc\\x8c\\xe6\\x97\\xa0\\xe5\\xb3\\xb0\\xe5\\xbd\\xa9\\xe9\\x98\\xb4\\xe7\\x91\\x81'\n",
      "Epoch 31/100\n",
      "109/109 [==============================] - 0s 277us/step - loss: 28.9058\n",
      "b'          \\xe5\\xb1\\x82\\xe5\\x8d\\xb3\\xe6\\xae\\xbd\\xe7\\x8e\\x89\\xe6\\x91\\x87\\xef\\xbc\\x8c\\xe6\\xac\\xa2\\xe5\\xae\\xb4\\xe8\\xa7\\x88\\xe7\\x92\\xa7\\xe5\\xbe\\x97'\n",
      "Epoch 32/100\n",
      "109/109 [==============================] - 0s 292us/step - loss: 25.7732\n",
      "b'          \\xe6\\xb8\\x85\\xe5\\x9d\\x9f\\xe6\\x97\\xa0\\xe6\\x85\\x8e\\xe6\\x97\\xa2\\xef\\xbc\\x8c\\xe9\\x83\\x91\\xe9\\x87\\x8d\\xe7\\x83\\x9f\\xe5\\xae\\xab\\xe5\\xb1\\x82'\n",
      "Epoch 33/100\n",
      "109/109 [==============================] - 0s 294us/step - loss: 24.6348\n",
      "b'          \\xe4\\xbd\\x95\\xe5\\x9d\\x9f\\xe8\\xb0\\x8f\\xe6\\xad\\xa4\\xe5\\xbf\\x85\\xef\\xbc\\x8c\\xe5\\xaf\\xb8\\xe7\\xad\\xb5\\xe9\\x9f\\xb3\\xe5\\x87\\xba\\xe9\\x9f\\xb3'\n",
      "Epoch 34/100\n",
      "109/109 [==============================] - 0s 308us/step - loss: 23.6632\n",
      "b'          \\xe8\\xa7\\x88\\xe7\\xbb\\xae\\xe6\\xac\\xb9\\xe5\\x8d\\xb1\\xe7\\x99\\xbe\\xef\\xbc\\x8c\\xe6\\x8a\\xab\\xe7\\x8e\\x89\\xe5\\x8d\\xb1\\xe5\\x85\\xb0\\xe6\\xad\\xa4'\n",
      "Epoch 35/100\n",
      "109/109 [==============================] - 0s 299us/step - loss: 22.9322\n",
      "b'          \\xe6\\xae\\xbf\\xe8\\x8a\\xb3\\xe9\\x83\\x91\\xe5\\x8d\\xab\\xe9\\x80\\x9a\\xef\\xbc\\x8c\\xe4\\xb8\\x8a\\xe6\\x80\\x80\\xe6\\x8a\\xab\\xe5\\x8d\\x83\\xe9\\x98\\xb3'\n",
      "Epoch 36/100\n",
      "109/109 [==============================] - 0s 299us/step - loss: 20.2676\n",
      "b'          \\xe9\\x9b\\xaa\\xe6\\x9b\\xb2\\xe5\\xbf\\x98\\xe9\\x9b\\xaa\\xe4\\xb9\\x83\\xef\\xbc\\x8c\\xe5\\x85\\xb9\\xe6\\x89\\x80\\xe9\\x98\\x85\\xe5\\x8d\\xb1\\xe8\\xba\\xac'\n",
      "Epoch 37/100\n",
      "109/109 [==============================] - 0s 301us/step - loss: 19.8778\n",
      "b'          \\xe5\\x8f\\xaf\\xe9\\xa9\\xac\\xe9\\xa3\\x8e\\xe5\\x8d\\xab\\xe7\\x99\\xbe\\xef\\xbc\\x8c\\xe6\\x98\\x8e\\xe7\\x92\\xa7\\xe7\\xa2\\xa7\\xe4\\xb9\\xb1\\xe7\\xa7\\x91'\n",
      "Epoch 38/100\n",
      "109/109 [==============================] - 0s 294us/step - loss: 18.1039\n",
      "b'          \\xe9\\x80\\x9a\\xe7\\xbb\\xb3\\xe6\\xb1\\x89\\xe4\\xba\\x8e\\xe6\\x9c\\xb1\\xef\\xbc\\x8c\\xe7\\x8e\\x89\\xe5\\x87\\xba\\xe6\\x8e\\xa5\\xe6\\x96\\xb9\\xe6\\x8e\\xa5'\n",
      "Epoch 39/100\n",
      "109/109 [==============================] - 0s 303us/step - loss: 19.0652\n",
      "b'          \\xe6\\x8e\\xa5\\xe9\\x9b\\x85\\xe6\\x96\\xb9\\xe5\\xa4\\x84\\xe7\\x99\\xbe\\xef\\xbc\\x8c\\xe5\\xbc\\x93\\xe5\\x85\\xb9\\xe7\\xa2\\xa7\\xe7\\x83\\x9f\\xe9\\x98\\xb3'\n",
      "Epoch 40/100\n",
      "109/109 [==============================] - 0s 311us/step - loss: 18.5626\n",
      "b'          \\xe6\\xac\\xa2\\xe7\\xaf\\x86\\xe5\\x96\\x84\\xe7\\xbb\\xae\\xe7\\x99\\xbd\\xef\\xbc\\x8c\\xe6\\xb1\\xbe\\xe7\\x92\\xa7\\xe5\\x8a\\xb3\\xe5\\xaf\\x9f\\xe9\\xa6\\x80'\n",
      "Epoch 41/100\n",
      "109/109 [==============================] - 0s 294us/step - loss: 19.1891\n",
      "b'          \\xe6\\xb1\\x89\\xe5\\xb9\\xbf\\xe5\\xbf\\x98\\xe4\\xb9\\x83\\xe5\\x8a\\xb3\\xef\\xbc\\x8c\\xe7\\x99\\xbe\\xe7\\xad\\xb5\\xe5\\x85\\xb0\\xe9\\xa3\\x8e\\xe5\\x8d\\xb7'\n",
      "Epoch 42/100\n",
      "109/109 [==============================] - 0s 281us/step - loss: 17.6517\n",
      "b'          \\xe6\\x9c\\xb1\\xe5\\x8a\\xbf\\xe6\\xa1\\xa5\\xe5\\xa6\\x82\\xe6\\xb1\\x89\\xef\\xbc\\x8c\\xe5\\x85\\xb9\\xe6\\x9c\\x88\\xe7\\xaf\\x86\\xe7\\x83\\x9f\\xe5\\xae\\xb4'\n",
      "Epoch 43/100\n",
      "109/109 [==============================] - 0s 315us/step - loss: 16.3119\n",
      "b'          \\xe5\\xbe\\x80\\xe5\\xbc\\xa6\\xe8\\xb0\\x8f\\xe8\\x8a\\xac\\xe7\\xbb\\xb3\\xef\\xbc\\x8c\\xe8\\xaf\\x9a\\xe9\\x9f\\xb3\\xe5\\xbf\\x85\\xe5\\x87\\x9d\\xe5\\x85\\xb9'\n",
      "Epoch 44/100\n",
      "109/109 [==============================] - 0s 315us/step - loss: 15.1685\n",
      "b'          \\xe9\\x95\\xbf\\xe8\\xb5\\x8f\\xe6\\x89\\x80\\xe8\\x99\\x9a\\xe4\\xb8\\xba\\xef\\xbc\\x8c\\xe4\\xb8\\x8a\\xe6\\xb1\\xa0\\xe5\\xbd\\xa9\\xe4\\xba\\xad\\xe5\\x8d\\xb1'\n",
      "Epoch 45/100\n",
      "109/109 [==============================] - 0s 302us/step - loss: 15.8539\n",
      "b'          \\xe5\\x87\\xba\\xe6\\x96\\x9c\\xe9\\x9a\\x90\\xe5\\x88\\x91\\xe5\\xbe\\x97\\xef\\xbc\\x8c\\xe9\\xa3\\x8e\\xe7\\xa2\\xa7\\xe4\\xba\\x8e\\xe7\\xbe\\x8e\\xe6\\x91\\x87'\n",
      "Epoch 46/100\n",
      "109/109 [==============================] - 0s 298us/step - loss: 16.3892\n",
      "b'          \\xe5\\xbe\\x97\\xe9\\x9b\\x89\\xe6\\x8e\\xa5\\xe4\\xb9\\xb1\\xe5\\xbf\\x98\\xef\\xbc\\x8c\\xe6\\x97\\xa5\\xe5\\xaf\\xb8\\xe5\\xb8\\xad\\xe5\\x96\\x84\\xe7\\x92\\xa7'\n",
      "Epoch 47/100\n",
      "109/109 [==============================] - 0s 280us/step - loss: 15.1186\n",
      "b'          \\xe5\\x87\\x9d\\xe5\\xa6\\x82\\xe6\\x89\\x80\\xe8\\xb0\\x8f\\xe5\\xbe\\x80\\xef\\xbc\\x8c\\xe9\\xa9\\xac\\xe7\\xbb\\xae\\xe7\\xaf\\x86\\xe9\\x9f\\xb3\\xe6\\xb3\\x9b'\n",
      "Epoch 48/100\n",
      "109/109 [==============================] - 0s 291us/step - loss: 15.7719\n",
      "b'          \\xe8\\x82\\x86\\xe8\\x90\\x8d\\xe6\\xb7\\xb3\\xe7\\xa2\\xa7\\xe5\\x86\\x99\\xef\\xbc\\x8c\\xe5\\xbf\\x85\\xe7\\x94\\xb5\\xe4\\xba\\xad\\xe5\\x85\\xb0\\xe7\\xa2\\xa7'\n",
      "Epoch 49/100\n",
      "109/109 [==============================] - 0s 298us/step - loss: 15.6345\n",
      "b'          \\xe6\\x8a\\xab\\xe5\\xaf\\xb9\\xe7\\xa6\\xbb\\xe6\\x8a\\xab\\xe7\\x8e\\xb3\\xef\\xbc\\x8c\\xe6\\x9c\\x88\\xe7\\x83\\x9f\\xe9\\x9a\\x90\\xe6\\x97\\xa0\\xe5\\x8d\\xab'\n",
      "Epoch 50/100\n",
      "109/109 [==============================] - 0s 316us/step - loss: 14.5154\n",
      "b'          \\xe5\\x80\\xa6\\xe9\\xa6\\x99\\xe8\\xb5\\x8f\\xe5\\x88\\x91\\xe7\\x99\\xbd\\xef\\xbc\\x8c\\xe5\\x85\\xb9\\xe4\\xba\\x91\\xe8\\xbe\\x99\\xe7\\x9a\\x8e\\xe6\\x96\\xb9'\n",
      "Epoch 51/100\n",
      "109/109 [==============================] - 0s 316us/step - loss: 15.3162\n",
      "b'          \\xe6\\x8a\\xab\\xe5\\x85\\xb8\\xe5\\x8a\\xb3\\xe9\\x83\\x91\\xe9\\x98\\x85\\xef\\xbc\\x8c\\xe5\\xaf\\xb9\\xe4\\xb9\\x83\\xe9\\x83\\x91\\xe9\\xa6\\x80\\xe7\\x8e\\xb3'\n",
      "Epoch 52/100\n",
      "109/109 [==============================] - 0s 299us/step - loss: 14.6451\n",
      "b'          \\xe7\\x96\\x8f\\xe7\\x8e\\x89\\xe7\\x91\\xb6\\xe5\\xb9\\xbf\\xe5\\xb1\\x82\\xef\\xbc\\x8c\\xe5\\xbf\\x97\\xe9\\x98\\x99\\xe7\\xba\\xb3\\xe5\\x9b\\xbe\\xe5\\xbe\\x85'\n",
      "Epoch 53/100\n",
      "109/109 [==============================] - 0s 305us/step - loss: 14.7666\n",
      "b'          \\xe5\\xbc\\x93\\xe8\\x80\\xbf\\xe6\\x97\\xa5\\xe9\\x85\\x92\\xe6\\x8e\\xa5\\xef\\xbc\\x8c\\xe8\\x82\\x86\\xe5\\x88\\x9d\\xe6\\x97\\xa2\\xe9\\x9b\\xaa\\xe5\\x8f\\xaf'\n",
      "Epoch 54/100\n",
      "109/109 [==============================] - 0s 305us/step - loss: 15.1714\n",
      "b'          \\xe6\\x80\\x80\\xe9\\x9f\\xb5\\xe5\\xaf\\x9f\\xe6\\x96\\xb9\\xe6\\xbe\\x84\\xef\\xbc\\x8c\\xe5\\x87\\x9d\\xe9\\x80\\x9a\\xe5\\xb1\\x82\\xe8\\x8d\\xa1\\xe9\\xa6\\x80'\n",
      "Epoch 55/100\n",
      "109/109 [==============================] - 0s 322us/step - loss: 14.8725\n",
      "b'          \\xe5\\x8c\\xa3\\xe9\\x9b\\x89\\xe5\\xbe\\x80\\xe6\\xb1\\x89\\xe5\\xb1\\x82\\xef\\xbc\\x8c\\xe7\\xaf\\x86\\xe7\\x96\\x91\\xe8\\x90\\x8d\\xe9\\x85\\x92\\xe5\\xaf\\xb8'\n",
      "Epoch 56/100\n",
      "109/109 [==============================] - 0s 310us/step - loss: 14.8214\n",
      "b'          \\xe5\\x8c\\xa3\\xe5\\xbf\\x98\\xe4\\xb9\\x83\\xe7\\xaf\\x86\\xe6\\x8a\\xab\\xef\\xbc\\x8c\\xe8\\x80\\xbf\\xe7\\x83\\x9f\\xe5\\xbe\\x85\\xe5\\x87\\xba\\xe7\\xae\\xa1'\n",
      "Epoch 57/100\n",
      "109/109 [==============================] - 0s 304us/step - loss: 15.6595\n",
      "b'          \\xe7\\xaf\\x86\\xe6\\xb1\\x89\\xe7\\x94\\xb5\\xe5\\x8a\\xb3\\xe8\\xa7\\x82\\xef\\xbc\\x8c\\xe9\\xbe\\x99\\xe9\\xa3\\x8e\\xe5\\x8d\\xb1\\xe8\\xa7\\x82\\xe6\\x97\\xa5'\n",
      "Epoch 58/100\n",
      "109/109 [==============================] - 0s 276us/step - loss: 14.5968\n",
      "b'          \\xe5\\x87\\xba\\xe4\\xbd\\x95\\xe4\\xba\\xba\\xe5\\xaf\\xb8\\xe5\\xb9\\xbf\\xef\\xbc\\x8c\\xe5\\xb3\\xb0\\xe9\\x9f\\xb3\\xe6\\x8e\\xa5\\xe7\\xaf\\x86\\xe9\\x87\\x8d'\n",
      "Epoch 59/100\n",
      "109/109 [==============================] - 0s 305us/step - loss: 14.3225\n",
      "b'          \\xe9\\x98\\xb4\\xe7\\x83\\x9f\\xe9\\xaa\\x8f\\xe6\\xb8\\x85\\xe6\\xae\\xbf\\xef\\xbc\\x8c\\xe6\\xb1\\xbe\\xe7\\xaf\\x86\\xe5\\xaf\\xbb\\xe9\\x95\\xbf\\xe9\\xa6\\x80'\n",
      "Epoch 60/100\n",
      "109/109 [==============================] - 0s 277us/step - loss: 14.6639\n",
      "b'          \\xe5\\xbe\\x80\\xe4\\xba\\x91\\xe4\\xbd\\x95\\xe6\\x8e\\xa5\\xe6\\xae\\xbf\\xef\\xbc\\x8c\\xe5\\xbf\\x98\\xe6\\x97\\xa0\\xe9\\x9b\\x89\\xe5\\xbf\\x98\\xe9\\x9b\\xaa'\n",
      "Epoch 61/100\n",
      "109/109 [==============================] - 0s 306us/step - loss: 13.3182\n",
      "b'          \\xe5\\x8d\\x83\\xe6\\x96\\xb9\\xe9\\x98\\xb3\\xe7\\x94\\xb5\\xe5\\x86\\x99\\xef\\xbc\\x8c\\xe7\\x8e\\x89\\xe5\\xb0\\xba\\xe7\\x96\\x8f\\xe5\\xbe\\x97\\xe7\\x99\\xbe'\n",
      "Epoch 62/100\n",
      "109/109 [==============================] - 0s 319us/step - loss: 14.9566\n",
      "b'          \\xe5\\xa4\\x9a\\xe6\\x96\\x9c\\xe5\\x80\\xa6\\xe8\\xb0\\x8f\\xe6\\xae\\xbf\\xef\\xbc\\x8c\\xe8\\x80\\xbf\\xe5\\x80\\xa6\\xe7\\xbb\\xae\\xe6\\x8a\\x9a\\xe9\\x83\\x91'\n",
      "Epoch 63/100\n",
      "109/109 [==============================] - 0s 316us/step - loss: 13.0047\n",
      "b'          \\xe5\\x89\\x8d\\xe7\\xbe\\x8e\\xe4\\xb9\\x83\\xe5\\xbf\\x97\\xe8\\xba\\xac\\xef\\xbc\\x8c\\xe6\\x96\\x9c\\xe5\\xbf\\x97\\xe6\\x9c\\xb1\\xe5\\xbf\\x83\\xe5\\xb1\\x82'\n",
      "Epoch 64/100\n",
      "109/109 [==============================] - 0s 320us/step - loss: 13.8588\n",
      "b'          \\xe5\\xaf\\xb8\\xe5\\x8d\\xb1\\xe6\\x96\\x9c\\xe8\\x82\\x86\\xe7\\xa6\\xbb\\xef\\xbc\\x8c\\xe5\\xaf\\xb8\\xe4\\xba\\x91\\xe9\\x9b\\x95\\xe5\\xb9\\xbf\\xe6\\xac\\xa2'\n",
      "Epoch 65/100\n",
      "109/109 [==============================] - 0s 296us/step - loss: 13.0550\n",
      "b'          \\xe4\\xbb\\x99\\xe9\\x95\\xbf\\xe4\\xb9\\x83\\xe6\\xb1\\xa0\\xe9\\x98\\x85\\xef\\xbc\\x8c\\xe4\\xbb\\x99\\xe7\\x91\\xb6\\xe7\\x92\\xa7\\xe6\\x97\\xa0\\xe7\\xbd\\x8d'\n",
      "Epoch 66/100\n",
      "109/109 [==============================] - 0s 310us/step - loss: 14.4979\n",
      "b'          \\xe6\\x81\\xb6\\xe8\\xb5\\x8f\\xe9\\x95\\xbf\\xe7\\x8e\\x89\\xe4\\xb8\\xbe\\xef\\xbc\\x8c\\xe6\\x97\\xa5\\xe5\\xaf\\x9f\\xe6\\x89\\x80\\xe5\\x89\\x8d\\xe8\\x8a\\xb3'\n",
      "Epoch 67/100\n",
      "109/109 [==============================] - 0s 316us/step - loss: 14.0065\n",
      "b'          \\xe7\\x8e\\x89\\xe6\\x97\\xa2\\xe7\\xaf\\x86\\xe6\\x97\\xa2\\xe9\\xa9\\xac\\xef\\xbc\\x8c\\xe9\\x9a\\x90\\xe9\\x87\\x91\\xe5\\xbe\\x85\\xe7\\xad\\xb5\\xe9\\xa6\\x99'\n",
      "Epoch 68/100\n",
      "109/109 [==============================] - 0s 323us/step - loss: 13.6525\n",
      "b'          \\xe5\\xa4\\x84\\xe7\\xbb\\xae\\xe6\\xb8\\x85\\xe5\\xbd\\xa9\\xe6\\xb3\\x9b\\xef\\xbc\\x8c\\xe6\\xad\\xa4\\xe7\\x8e\\x89\\xe6\\x97\\xa2\\xe5\\x85\\xb0\\xe7\\xa2\\xa7'\n",
      "Epoch 69/100\n",
      "109/109 [==============================] - 0s 298us/step - loss: 12.9801\n",
      "b'          \\xe9\\xa9\\xac\\xe6\\x97\\xa0\\xe5\\xbf\\x98\\xe8\\x80\\xbf\\xe6\\x8a\\x9a\\xef\\xbc\\x8c\\xe8\\xb8\\xaa\\xe5\\x8e\\xbb\\xe7\\xbb\\xae\\xe7\\xa7\\x91\\xe7\\x94\\xb5'\n",
      "Epoch 70/100\n",
      "109/109 [==============================] - 0s 316us/step - loss: 12.7175\n",
      "b'          \\xe9\\xa3\\x8e\\xe5\\x90\\xaf\\xe6\\xb8\\x85\\xe5\\x8d\\xb1\\xe5\\x9d\\x9f\\xef\\xbc\\x8c\\xe9\\x98\\x99\\xe4\\xb8\\xbe\\xe5\\x80\\xa6\\xe5\\x8e\\xbb\\xe6\\x8a\\xab'\n",
      "Epoch 71/100\n",
      "109/109 [==============================] - 0s 315us/step - loss: 12.4061\n",
      "b'          \\xe6\\x97\\xa5\\xe9\\x9b\\xaa\\xe5\\x8d\\xb1\\xe5\\x80\\xa6\\xe6\\xb8\\x85\\xef\\xbc\\x8c\\xe6\\x80\\xa5\\xe9\\x99\\x88\\xe5\\x86\\x99\\xe5\\xbf\\x97\\xe5\\x8a\\xb3'\n",
      "Epoch 72/100\n",
      "109/109 [==============================] - 0s 292us/step - loss: 13.9302\n",
      "b'          \\xe9\\x95\\xbf\\xe4\\xbd\\x95\\xe5\\x87\\xa4\\xe9\\x99\\x88\\xe4\\xb9\\x83\\xef\\xbc\\x8c\\xe8\\xb5\\x8f\\xe7\\x9b\\x88\\xe9\\x98\\xb4\\xe7\\xbb\\xae\\xe5\\x80\\xa6'\n",
      "Epoch 73/100\n",
      "109/109 [==============================] - 0s 301us/step - loss: 13.1935\n",
      "b'          \\xe5\\xb2\\x82\\xe5\\x9d\\x9f\\xe6\\x9c\\xb1\\xe7\\xae\\xa1\\xe9\\x9b\\xaa\\xef\\xbc\\x8c\\xe5\\x97\\xa3\\xe4\\xb8\\xbe\\xe7\\xbb\\xae\\xe6\\xb2\\xb3\\xe6\\xad\\xa4'\n",
      "Epoch 74/100\n",
      "109/109 [==============================] - 0s 257us/step - loss: 13.8847\n",
      "b'          \\xe9\\x85\\x92\\xe5\\xbe\\x80\\xe7\\xbe\\x8e\\xe5\\xb9\\xbf\\xe5\\xbe\\x80\\xef\\xbc\\x8c\\xe5\\xbf\\x83\\xe7\\x94\\xb5\\xe9\\x9f\\xb3\\xe5\\x9d\\x9f\\xe6\\x9c\\xb1'\n",
      "Epoch 75/100\n",
      "109/109 [==============================] - 0s 312us/step - loss: 12.9147\n",
      "b'          \\xe5\\xbf\\x97\\xe6\\x98\\x8e\\xe7\\x8e\\xb3\\xe8\\x99\\x9a\\xe7\\xbb\\xb3\\xef\\xbc\\x8c\\xe5\\xb3\\xb0\\xe6\\x95\\xa3\\xe7\\xbb\\xb3\\xe7\\xbb\\xb3\\xe6\\x8e\\xa5'\n",
      "Epoch 76/100\n",
      "109/109 [==============================] - 0s 298us/step - loss: 13.6729\n",
      "b'          \\xe4\\xbb\\x99\\xe8\\xa7\\x82\\xe9\\x9a\\x90\\xe9\\x87\\x8d\\xe5\\xb9\\xbf\\xef\\xbc\\x8c\\xe6\\xb5\\x81\\xe5\\x8e\\xbb\\xe9\\x9f\\xb3\\xe7\\xae\\xa1\\xe5\\x80\\xa6'\n",
      "Epoch 77/100\n",
      "109/109 [==============================] - 0s 302us/step - loss: 12.3103\n",
      "b'          \\xe6\\xae\\xbf\\xe4\\xbd\\x95\\xe5\\xbf\\x98\\xe8\\x8d\\xa1\\xe8\\xb8\\xaa\\xef\\xbc\\x8c\\xe7\\x99\\xbe\\xe7\\xbe\\x8e\\xe5\\x8d\\x83\\xe5\\x87\\x9d\\xe7\\xa7\\x91'\n",
      "Epoch 78/100\n",
      "109/109 [==============================] - 0s 297us/step - loss: 12.6177\n",
      "b'          \\xe5\\xaf\\x9f\\xe5\\xbe\\x80\\xe6\\x98\\xad\\xe6\\x81\\xb6\\xe5\\x80\\xa6\\xef\\xbc\\x8c\\xe7\\xb4\\xa0\\xe4\\xbb\\x99\\xe5\\xb0\\xba\\xe5\\x8d\\xb7\\xe6\\x8a\\xab'\n",
      "Epoch 79/100\n",
      "109/109 [==============================] - 0s 304us/step - loss: 12.6342\n",
      "b'          \\xe5\\xbf\\x98\\xe7\\x91\\x81\\xe5\\x8e\\xbb\\xe9\\xa9\\xac\\xe8\\xa7\\x82\\xef\\xbc\\x8c\\xe7\\xaf\\x86\\xe5\\xa4\\x84\\xe4\\xba\\xba\\xe6\\xbe\\x84\\xe6\\xae\\xbf'\n",
      "Epoch 80/100\n",
      "109/109 [==============================] - 0s 256us/step - loss: 12.3014\n",
      "b'          \\xe9\\xab\\x98\\xe8\\xb8\\xaa\\xe7\\xbb\\xae\\xe5\\x80\\xa6\\xe5\\x9b\\xbe\\xef\\xbc\\x8c\\xe9\\x97\\xb4\\xe9\\xa6\\x80\\xe4\\xbb\\x99\\xe8\\xb5\\xb7\\xe8\\xb8\\xaa'\n",
      "Epoch 81/100\n",
      "109/109 [==============================] - 0s 318us/step - loss: 13.4674\n",
      "b'          \\xe7\\xa5\\x9e\\xe4\\xbd\\x95\\xe6\\xb8\\x85\\xe5\\xbe\\x97\\xe7\\x8e\\x89\\xef\\xbc\\x8c\\xe6\\xb1\\xa0\\xe5\\xbf\\x85\\xe7\\xa7\\x91\\xe6\\x8a\\x9a\\xe9\\x83\\x91'\n",
      "Epoch 82/100\n",
      "109/109 [==============================] - 0s 317us/step - loss: 12.6533\n",
      "b'          \\xe4\\xb8\\xbe\\xe9\\x87\\x8d\\xe5\\xaf\\xb8\\xe4\\xbd\\x95\\xe7\\xad\\xb5\\xef\\xbc\\x8c\\xe7\\x9a\\x8e\\xe5\\xa6\\x82\\xe7\\xaf\\x86\\xe6\\x96\\xb9\\xe7\\xa7\\x91'\n",
      "Epoch 83/100\n",
      "109/109 [==============================] - 0s 320us/step - loss: 12.6967\n",
      "b'          \\xe4\\xba\\x91\\xe7\\xae\\xa1\\xe5\\x9c\\x83\\xe6\\x89\\x80\\xe5\\xaf\\xbb\\xef\\xbc\\x8c\\xe6\\x97\\xa5\\xe5\\xaf\\xb9\\xe7\\xaf\\x86\\xe5\\xbd\\xa9\\xe7\\xbb\\xae'\n",
      "Epoch 84/100\n",
      "109/109 [==============================] - 0s 279us/step - loss: 13.3357\n",
      "b'          \\xe7\\x96\\x91\\xe5\\x88\\x91\\xe6\\xb8\\x85\\xe4\\xb8\\x8a\\xe6\\x8e\\xa5\\xef\\xbc\\x8c\\xe9\\x9a\\x90\\xe8\\xa7\\x88\\xe5\\xbf\\x97\\xe5\\xbd\\xa9\\xe9\\xa3\\x8e'\n",
      "Epoch 85/100\n",
      "109/109 [==============================] - 0s 299us/step - loss: 13.0728\n",
      "b'          \\xe6\\x97\\xa5\\xe6\\xb8\\x85\\xe6\\xb8\\x85\\xe8\\x99\\x9a\\xe5\\xaf\\xb9\\xef\\xbc\\x8c\\xe6\\x97\\xa5\\xe7\\x96\\x91\\xe9\\x83\\x91\\xe6\\xb1\\xbe\\xe5\\xbe\\x97'\n",
      "Epoch 86/100\n",
      "109/109 [==============================] - 0s 320us/step - loss: 11.5755\n",
      "b'          \\xe6\\xac\\xa2\\xe6\\xbe\\x84\\xe6\\xb1\\xbe\\xe9\\x95\\xbf\\xe7\\x83\\x9f\\xef\\xbc\\x8c\\xe5\\xb3\\xb0\\xe9\\x87\\x8d\\xe5\\x85\\xb0\\xe7\\x83\\x9f\\xe9\\x9f\\xb3'\n",
      "Epoch 87/100\n",
      "109/109 [==============================] - 0s 308us/step - loss: 12.8267\n",
      "b'          \\xe6\\x8a\\xab\\xe9\\x98\\x85\\xe5\\x85\\xb8\\xe6\\xb3\\x9b\\xe5\\xae\\xb4\\xef\\xbc\\x8c\\xe9\\x98\\x81\\xe7\\xad\\xb5\\xe6\\x8a\\x9a\\xe6\\xb2\\xb3\\xe8\\xa7\\x88'\n",
      "Epoch 88/100\n",
      "109/109 [==============================] - 0s 323us/step - loss: 13.2050\n",
      "b'          \\xe5\\x90\\xaf\\xe9\\x83\\x91\\xe5\\x8d\\xb1\\xe8\\x8d\\xb7\\xe5\\xb1\\x82\\xef\\xbc\\x8c\\xe7\\xaf\\x86\\xe5\\x80\\xa6\\xe5\\x86\\x99\\xe5\\x80\\xa6\\xe6\\x96\\xb9'\n",
      "Epoch 89/100\n",
      "109/109 [==============================] - 0s 317us/step - loss: 13.8328\n",
      "b'          \\xe8\\xb5\\x8f\\xe4\\xb9\\x83\\xe7\\xaf\\x86\\xe7\\xae\\xa1\\xe9\\x81\\x93\\xef\\xbc\\x8c\\xe5\\x89\\x8d\\xe7\\x99\\xbe\\xe9\\x95\\xbf\\xe6\\xb7\\xb3\\xe6\\x95\\xa3'\n",
      "Epoch 90/100\n",
      "109/109 [==============================] - 0s 316us/step - loss: 11.5142\n",
      "b'          \\xe5\\xaf\\x9f\\xe6\\x80\\xa5\\xe5\\xbf\\x98\\xe6\\xb8\\x85\\xe6\\x96\\xb9\\xef\\xbc\\x8c\\xe5\\xb8\\xad\\xe9\\x9a\\x90\\xe9\\x9f\\xb3\\xe5\\xbf\\x98\\xe9\\x9f\\xb3'\n",
      "Epoch 91/100\n",
      "109/109 [==============================] - 0s 314us/step - loss: 10.4517\n",
      "b'          \\xe6\\x89\\x80\\xe7\\xa2\\xa7\\xe7\\xa6\\xbb\\xe5\\x8d\\xb1\\xe6\\xb8\\x85\\xef\\xbc\\x8c\\xe7\\x8e\\xb3\\xe6\\x8e\\xa5\\xe8\\x8d\\xb7\\xe7\\x83\\x9f\\xe6\\x97\\xa2'\n",
      "Epoch 92/100\n",
      "109/109 [==============================] - 0s 318us/step - loss: 12.4240\n",
      "b'          \\xe8\\xbe\\x99\\xe5\\x8d\\xb1\\xe5\\xaf\\xb9\\xe6\\x97\\xa2\\xe5\\xbe\\x85\\xef\\xbc\\x8c\\xe6\\xb1\\xa0\\xe9\\xa3\\x8e\\xe8\\xba\\xac\\xe7\\x9a\\x8e\\xe6\\x8a\\xab'\n",
      "Epoch 93/100\n",
      "109/109 [==============================] - 0s 326us/step - loss: 11.2051\n",
      "b'          \\xe8\\xb5\\x8f\\xe6\\xb7\\xb9\\xe6\\x97\\xa2\\xe5\\x8a\\xbf\\xe8\\xb0\\x8f\\xef\\xbc\\x8c\\xe7\\x8e\\xb3\\xe8\\x99\\x9a\\xe6\\x97\\xa5\\xe4\\xbd\\x95\\xe5\\xbf\\x97'\n",
      "Epoch 94/100\n",
      "109/109 [==============================] - 0s 310us/step - loss: 13.1154\n",
      "b'          \\xe5\\x8e\\xbb\\xe7\\xa7\\x91\\xe4\\xb8\\x8a\\xe6\\xae\\xbd\\xe5\\xb2\\x82\\xef\\xbc\\x8c\\xe7\\xbb\\xae\\xe9\\x9a\\x90\\xe9\\xa6\\x99\\xe6\\x95\\xa3\\xe7\\xa7\\x91'\n",
      "Epoch 95/100\n",
      "109/109 [==============================] - 0s 306us/step - loss: 14.1563\n",
      "b'          \\xe4\\xb9\\xb1\\xe8\\x82\\x86\\xe5\\xb0\\xba\\xe5\\x97\\xa3\\xe6\\xb8\\x85\\xef\\xbc\\x8c\\xe7\\x96\\x91\\xe7\\xa2\\xa7\\xe9\\x98\\x85\\xe7\\xbb\\xb3\\xe6\\x80\\x80'\n",
      "Epoch 96/100\n",
      "109/109 [==============================] - 0s 299us/step - loss: 12.6334\n",
      "b'          \\xe5\\x80\\xa6\\xe5\\xb9\\xbf\\xe6\\x82\\xac\\xe5\\xbf\\x98\\xe6\\x8a\\xab\\xef\\xbc\\x8c\\xe5\\xbf\\x85\\xe7\\x99\\xbd\\xe7\\xae\\xa1\\xe8\\x80\\xbf\\xe7\\xa2\\xa7'\n",
      "Epoch 97/100\n",
      "109/109 [==============================] - 0s 310us/step - loss: 13.3289\n",
      "b'          \\xe4\\xb9\\xb1\\xe9\\x9f\\xb5\\xe6\\xae\\xbf\\xe5\\x8f\\xaf\\xe7\\xbb\\xb3\\xef\\xbc\\x8c\\xe5\\x8c\\xa3\\xe7\\xa7\\x91\\xe9\\x9b\\x89\\xe9\\xa9\\xac\\xe5\\xaf\\xb8'\n",
      "Epoch 98/100\n",
      "109/109 [==============================] - 0s 314us/step - loss: 12.2127\n",
      "b'          \\xe6\\x97\\xa2\\xe5\\x8d\\xb3\\xe9\\x9a\\x90\\xe7\\x92\\xa7\\xe5\\x87\\x9d\\xef\\xbc\\x8c\\xe7\\x83\\x9f\\xe9\\x9f\\xb3\\xe7\\x99\\xbd\\xe5\\xb2\\x82\\xe5\\x80\\xa6'\n",
      "Epoch 99/100\n",
      "109/109 [==============================] - 0s 317us/step - loss: 11.6300\n",
      "b'          \\xe9\\x87\\x91\\xe6\\x8a\\xab\\xe5\\xbf\\x98\\xe5\\xaf\\xbb\\xe6\\x97\\xa2\\xef\\xbc\\x8c\\xe6\\xb1\\xa0\\xe6\\x98\\xad\\xe8\\xb5\\x8f\\xe6\\x97\\xa5\\xe8\\xb5\\x8f'\n",
      "Epoch 100/100\n",
      "109/109 [==============================] - 0s 318us/step - loss: 11.7683\n",
      "b'          \\xe9\\x80\\x9a\\xe5\\xaf\\xbb\\xe7\\x92\\xa7\\xe9\\x9f\\xb3\\xe8\\xb5\\x8f\\xef\\xbc\\x8c\\xe6\\xb1\\xa0\\xe6\\x98\\xad\\xe6\\x8e\\xa5\\xe8\\xba\\xac\\xe7\\xaf\\x86'\n",
      "披声寻绮峰，素篆芳耿池\n",
      "日汉朱察寻，忘披纳汉疑\n",
      "朱倦清忘尺，欢篆篆雕岂\n",
      "朱席音兹踪，寸兹起烟起\n",
      "殿阴长隐乃，赏馀往既启\n",
      "诚斜圃隐往，化无纳肆阅\n",
      "览弦广疑处，诚日月长怀\n",
      "玉云写倦寻，阙马赏骏风\n",
      "殿日忘里响，瑶诚玉览风\n",
      "躬乃香岂芳，善去往可玉\n",
      "去踪典倦绮，响去韵前广\n",
      "肆弦散亭广，宴阴乃绮广\n",
      "月雪声无篆，日披月赏绮\n",
      "殿何玳纳云，必悦迹得殽\n",
      "匣烟桥劳科，层所绳云间\n",
      "恶音忘里写，披匣日得疏\n",
      "乃察寸乱出，明池兹抚观\n",
      "殿池阳兹间，疑可绮欹写\n",
      "兰罍绮阙轻，兰赏玉处忠\n",
      "出雪乃所所，朱管广美绮\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import re\n",
    "import codecs\n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "n = 5 # 只抽取五言诗\n",
    "latent_dim = 64 # 隐变量维度\n",
    "hidden_dim = 64 # 隐层节点数\n",
    "\n",
    "## Preprocessor.get_shi()\n",
    "s = codecs.open('shi.txt', encoding='utf-8').read()\n",
    "# for i in s:\n",
    "#     print(i)\n",
    "# 通过正则表达式找出所有的五言诗\n",
    "#  re.compile(r'foo\\(.*?\\)')\n",
    "# s = re.findall(u'(.{%s}，.{%s}。.)\\r\\n'%(n,n), s)\n",
    "s = re.findall(u'([^]+)。([^]+)。\\n', s)\n",
    "# for i in s:\n",
    "#     print(i)\n",
    "shi = []\n",
    "for i in s:\n",
    "#     print(i)\n",
    "    for j in i.split(u'。'): # 按句切分\n",
    "        if j:\n",
    "            shi.append(j)\n",
    "\n",
    "shi = [i[:n] + i[n+1:] for i in shi if len(i) == 2*n+1]\n",
    "print(len(shi))\n",
    "print(len(s))\n",
    "\n",
    "## Preprocessor.get_vocab()\n",
    "# 构建字与id的相互映射\n",
    "id2char = dict(enumerate(set(''.join(shi))))\n",
    "char2id = {j:i for i,j in id2char.items()}\n",
    "print(f'length {len(id2char)}')\n",
    "embedding_len = len(id2char)\n",
    "\n",
    "# 诗歌id化\n",
    "shi2id = [[char2id[j] for j in i] for i in shi]\n",
    "shi2id = np.array(shi2id)\n",
    "\n",
    "# Normolization\n",
    "# print(type(shi2id[0][0]))\n",
    "# Normalizing the images to the range of [0., 1.]\n",
    "# shi2id /= embedding_len\n",
    "# shi2id /= embedding_len\n",
    "\n",
    "print(type(shi2id[0][0]))\n",
    "\n",
    "# # Binarization\n",
    "# train_images[train_images >= .5] = 1.\n",
    "# train_images[train_images < .5] = 0.\n",
    "# test_images[test_images >= .5] = 1.\n",
    "# test_images[test_images < .5] = 0.\n",
    "\n",
    "# no flow control\n",
    "class GCNN(Layer): # 定义GCNN层，结合残差\n",
    "    def __init__(self, output_dim=None, residual=False, **kwargs):\n",
    "        super(GCNN, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.residual = residual\n",
    "    def build(self, input_shape):\n",
    "        if self.output_dim == None:\n",
    "            self.output_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(name='gcnn_kernel',\n",
    "                                     shape=(3, input_shape[-1],\n",
    "                                            self.output_dim * 2),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "    def call(self, x):\n",
    "        _ = K.conv1d(x, self.kernel, padding='same')\n",
    "        _ = _[:,:,:self.output_dim] * K.sigmoid(_[:,:,self.output_dim:])\n",
    "        if self.residual:\n",
    "            return _ + x\n",
    "        else:\n",
    "            return _\n",
    "\n",
    "## model.VAEModel\n",
    "input_sentence = Input(shape=(2*n,), dtype='int32')\n",
    "input_vec = Embedding(embedding_len, hidden_dim)(input_sentence) # id转向量\n",
    "h = GCNN(residual=True)(input_vec) # GCNN层\n",
    "h = GCNN(residual=True)(h) # GCNN层\n",
    "h = GCNN(residual=True)(h) # GCNN层\n",
    "h = GCNN(residual=True)(h) # GCNN层\n",
    "h = GCNN(residual=True)(h) # GCNN层\n",
    "h = GCNN(residual=True)(h) # GCNN层\n",
    "h = GCNN(residual=True)(h) # GCNN层\n",
    "h = GCNN(residual=True)(h) # GCNN层\n",
    "h = GCNN(residual=True)(h) # GCNN层\n",
    "h = GCNN(residual=True)(h) # GCNN层\n",
    "h = GlobalAveragePooling1D()(h) # 池化\n",
    "\n",
    "# 算均值方差\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "## modules.sampling\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0, stddev=1)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "z = Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "# 定义解码层，分开定义是为了后面的重用\n",
    "decoder_hidden = Dense(hidden_dim*(2*n))\n",
    "decoder_cnn = GCNN(residual=True)\n",
    "decoder_dense = Dense(len(char2id), activation='softmax')\n",
    "\n",
    "h = decoder_hidden(z)\n",
    "h = Reshape((2*n, hidden_dim))(h)\n",
    "h = decoder_cnn(h)\n",
    "h = decoder_cnn(h)\n",
    "h = decoder_cnn(h)\n",
    "h = decoder_cnn(h)\n",
    "h = decoder_cnn(h)\n",
    "h = decoder_cnn(h)\n",
    "h = decoder_cnn(h)\n",
    "h = decoder_cnn(h)\n",
    "h = decoder_cnn(h)\n",
    "h = decoder_cnn(h)\n",
    "output = decoder_dense(h)\n",
    "\n",
    "\n",
    "# 建立模型\n",
    "vae = Model(input_sentence, output)\n",
    "\n",
    "# xent_loss是重构loss，kl_loss是KL loss\n",
    "xent_loss = K.sum(K.sparse_categorical_crossentropy(input_sentence, output), 1)\n",
    "kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "vae_loss = K.mean(xent_loss + kl_loss)\n",
    "\n",
    "# add_loss是新增的方法，用于更灵活地添加各种loss\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()\n",
    "\n",
    "# 重用解码层，构建单独的生成模型\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_ = decoder_hidden(decoder_input)\n",
    "_ = Reshape((2*n, hidden_dim))(_)\n",
    "_ = decoder_cnn(_)\n",
    "_output = decoder_dense(_)\n",
    "generator = Model(decoder_input, _output)\n",
    "\n",
    "\n",
    "# 利用生成模型随机生成一首诗\n",
    "def gen():\n",
    "    r = generator.predict(np.random.randn(1, latent_dim))[0]\n",
    "    r = r.argmax(axis=1)\n",
    "    return ''.join([id2char[i] for i in r[:n]])\\\n",
    "           + u'，'\\\n",
    "           + ''.join([id2char[i] for i in r[n:]])\n",
    "\n",
    "\n",
    "# 回调器，方便在训练过程中输出\n",
    "class Evaluate(Callback):\n",
    "    def __init__(self):\n",
    "        self.log = []\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.log.append(gen())\n",
    "        print( (u'          %s'%(self.log[-1])).encode('utf-8') )\n",
    "\n",
    "\n",
    "evaluator = Evaluate()\n",
    "\n",
    "vae.fit(shi2id,\n",
    "        shuffle=True,\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        callbacks=[evaluator])\n",
    "\n",
    "vae.save_weights('shi.model')\n",
    "\n",
    "for i in range(20):\n",
    "    print(gen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
